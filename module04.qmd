# Module 4: Model Parameterization {#module-4 .unnumbered}

# General Description

This module aims to adjust the classifier’s model parameter using two different options: use existing model from other users or use system’s parameter recommendations. For Phase 1 of the development, these parameters are applied to train and evaluate the Random Forest (RF) model [@ho_random_1995].

![Module 4 workflow. View full-size image in new tab](images/module4-workflow.png)

|  |  |
|------------------------------------|------------------------------------|
| User Input | \- Hyperparameter tuning constraints<br>- Training/test data split ratio |
| Process | Random Forest classification (options: hard, soft, or hierarchical) |
| Output | Suggested optimal hyperparameters for the Random Forest model |

# Feature Notes

-   System consistency existing model compatibility with user’s defined class

-   Hyperparameter tuning

    The system provides an option to generate optimal parameters for the Random Forest algorithm. The parameters tuned by the system include the number of trees, variables per split, and minimum leaf population. The classification can be performed in two ways: **hard** or **soft** classification.

    In **hard classification**, the system applies a single-split mechanism where each pixel is assigned to exactly one class. In contrast, **soft classification** uses a one-vs-rest binary approach, where the classifier estimates the probability of a pixel belonging to each possible class instead of assigning it directly. For both methods, the system iterates through multiple combinations of Random Forest parameters to identify the best-performing set. Users may choose to rely on the system’s default tuning constraints or define their own parameters manually. In future module development, a **hierarchical classification** option will be added for multi-level class separation.

    The results of the hyperparameter tuning are presented as performance indices. For soft classification, the system reports log loss (cross-entropy), where a lower average cross-entropy across all classes indicates a better parameter set. For hard classification, model quality is evaluated based on accuracy against the test dataset.
